#Augst 5, 2017
#STEP 1: Downloading the Equity files 1887-1902
-in DH Box, created directory called Final
-in Final, created directory for Equity Papers, called Equity_Papers_1899
-I will put all my Equity papers in this folder
*I am going to use the papers from 1897-1902 (6 years) to see if there is an increase of French/English conflict in Canada from  before to after the Second Boer War begins in South Africa and there is a difference of opinion between the two groups about supporting the British troops. This will be especially interesting because without doing any research at this point, I suspect the Shawville Equity as an English paper within Quebec will have an interesting take on the matter.*
*Perhaps this will not even be in the papers, in which case I will focus on whatever the paper focuses on.*

*I recognize that six years worth of data is not a large sample, but I hope that the change will be great enough to notice over the six years. I also made this decision practically because I do not have unlimited internet and did not want to download a decade's worth of materials*

#Fail
-I used the weget command from Module 2, modifying it so it would take the 1897 files. I received the following error:

claremaier@3bcf141bb995:~/Final/Equity_Papers_1899$ wget http://collections.banq.qc.ca:8008
/jrn03/equity/src/1897/ -A .txt -r --no-parent -nd âw 2 --limit-rate=20k
--2017-08-05 21:57:09--  http://collections.banq.qc.ca:8008/jrn03/equity/src/188397-A
Resolving collections.banq.qc.ca (collections.banq.qc.ca)... 198.168.27.56
Connecting to collections.banq.qc.ca (collections.banq.qc.ca)|198.168.27.56|:8008... connec
ted.
HTTP request sent, awaiting response... 404 Not Found
2017-08-05 21:57:09 ERROR 404: Not Found.
--2017-08-05 21:57:09--  http://.txt/
Resolving .txt (.txt)... failed: Name or service not known.
wget: unable to resolve host address ‘.txt’
idn_encode failed (1): ‘String preparation failed’
idn_encode failed (1): ‘String preparation failed’
--2017-08-05 21:57:09--  http://%C3%A2%C2%80%C2%93w/
Resolving â\302\200\302\223w (â\302\200\302\223w)... failed: Name or service not known.
wget: unable to resolve host address ‘â\302\200\302\223w’
--2017-08-05 21:57:09--  http://2/
Resolving 2 (2)... 0.0.0.2
Connecting to 2 (2)|0.0.0.2|:80... failed: Invalid argument.

*I returned to the tutorial to see what I did wrong. It appears that I had to remove to "-A" and somehow I accidently added "âw" instead of just "w".* I replaced that command with:
wget -r --no-parent -w 2 --limit-rate=20k http://collections.banq.qc.ca:8008/jrn03/equity/src/1897/

-it appeared to be working well, but the files were downloading very very slowly and then they opened to the original website location. Not what I wanted, so I cancelled the download and tried again, this time without DH Box, and on my local computer.

I tried: wget -r --no-parent -w 2 --limit-rate=20k http://collections.banq.qc.ca:8008/jrn03/equity/src/1897/ -A .txt
*this finally worked - I am downloading each of the 6 years independently and verifying the files are complete and planted in the correct folders *

#August 6, 2017
#Step 2: Regex with Python **Attempt 1: Fail**
-I wanted to clean up the OCR data before I analysed it, so I looked
at the old exercises and identified TEI. That exercise looked like it
was for one document.
I found Regex and saw the link for the [Cleaning OCR'd Text with Regular Expressions](https://programminghistorian.org/lessons/cleaning-ocrd-text-with-regular-expressions)
I opened a text file and started to work off the python script provided in the tutorial. I changed the names of files, copied and pasted portions and tried to understand it.
When I pasted it in to the terminal, I received error message after error message.
They all basically looked like this: 
clare@clare-fun1:~/School/Final_EquityProject$ python PthyonScript.py
  File "PthyonScript.py", line 19
    nodash = re.sub('.(-+)', ',', line)
         ^
IndentationError: expected an indented block

My brother explained that indents need to be very precise, so I set about making sure they all lined up properly.
It still didn't work, so I just copied and pasted the whole script in and changed the file names.
Somehow terminal was working off a different version of my file PthyonScript.py (I also spelled it wrong) and wouldn't make the changes. 
When I made sure all the files were in the right place, I ran it again. It correctly filled the csv, but I realised I wanted a txt file. I ran it again and created the changed text in the text file.

When I opened the file, I found that the script had deleted 1700 lines of text, leaving me with 14. 

**What I learned**
I learned that python scripts are very exact and that I need to be very careful when I build them
I learned that carefully naming and locating files is key to getting work done quickly and without unnecessary frustration.
I learned that it is a bad idea to use other people's stuff without understanding it and making sure it works for my project.
I was reminded of how important it is to make backups (glad that I did). 

**Next steps**
Carefully examine several sample txt files, noting where I think the most common OCR mistakes are and then use python to carefully write useful regex expressions
My goal is to clean up the library (not sure if this is possible or if each one needs to be done automaticaly)
Then I want to do some topic modeling and see whatelse seems useful

*python script is in 1897/01/07*
<<<<<<< HEAD

#August 7, 2017
#Step 3: RStudio
I attempt to try out the readability with some of the Equity papers.
Because it was a new DH Box, I had to install rJava and Mallet again.

I ran the following commands, modified from their original [tutorial](http://workbook.craftingdigitalhistory.ca/supporting%20materials/topicmodel-r-dhbox/)

documents <- mallet.read.dir("/home/claremaier/Equity_Papers1897") >

I then tried to run
topic.model$loadDocuments(mallet.instances), 
but kept receiving this error: 
Error in lapply(list(...), ._java_valid_object) : 
  object 'mallet.instances' not found

I continued having similar errors:
 documents <- read.csv(text = x, col.names=c("Article_ID", "Newspaper Title", "Newspaper City", "Newspaper Province", "Newspaper Country", "Year", "Month", "Day", "Article Type", "Text", "Keywords"), colClasses=rep("character", 3), sep=",", quote="")
Error in textConnection(text, encoding = "UTF-8") : object 'x' not found
> topic.model$loadDocuments(mallet.instances)
Loading required package: rJava
Error in topic.model$loadDocuments : 
  $ operator not defined for this S4 class >

-I got very confused and didn't know exactly what I was doing. They I realized that I wasn't working with a csv file, which it appears that you have to in order for the program to work. I started thinking about making a csv and realized that it needed clear headings. My documents did not have clear headings. I tried to think of clear headings, but realized the data was too messy.
I went back to my original plan of cleaning the data, but this time decided to use regex. I could not find what felt like enough consistant errors to justify writing a regex expression.

I looked for other things to work on, because right then I felt pretty overwhelmed and worried. I found the TEI tutorial (which I wasn't able to work through the first time) and noted the Prof's note that doing this with an equity file would be an appropriate file. 
**the current plan is to use the TEI tutorial to markup an Equity file for searching. I would also like to take it a few steps further, but I'm not sure how right now.**
*I'm still having trouble understanding the project requirements - re: research and how it should look. The whole thing scares me because I'm having trouble conceptualizing it, so I'm going to just focus on the TEI and documenting it well at this point.* **perhaps open refine could help remove some of the incorrectly spelled, etc text**

#Step 4: TEI
I am following the tutorial from class, making changes as necessary.
[Tutorial](http://workbook.craftingdigitalhistory.ca/supporting%20materials/tei/)

I used the [blanktemplate.txt](https://github.com/craftingdigitalhistory/module3-wranglingdata/blob/master/tei-hist3907/blanktemplate.txt) before adding and modifying it.

Between <body> and </body>, I pasted in the 1800 lines of Equity file [83471_1897-01-14.txt](https://github.com/claremaier/Final_Project/blob/master/collections.banq.qc.ca:8008/jrn03/equity/src/1897/01/14/83471_1897-01-14.txt)

I then marked the beginning of every heading and paragraph with <p> and ended each heading or paragraph with </p>.

#Decisions
I had to make decisions about where to make the paragraph breaks throughout the text. In some cases, it wasn't clear, so I chose to enclose the unreadable text within its own section, so that clear portions continued to be clear. Note: I did not take this approach if the unclear sections were within clear sections.
Sometimes it is difficult to tell if two topics have been merged by the OCR, so I do my best to group it all together - can manually go through it later

<p>1 hey are <>lltif remarks that the continued absence I Imi*ohtant to Farmer*.L. D. Davis, being held in too school house and ®r, of snow there is likely to prove a a**ri I ^ Shaw ville, has been operating a de* to till a 1«mg fuit want in that di«t our matter to the operations of lumber j horning machine in tins section for some trict. A church will likely be built there men jn that locality.	time P#t with great success. All who
in the spring.	|	I have had their cattle dehorned are per-
yean, and 10 month*. Mis remains were ,,f th 0ttawa Hou.ti, win, will conduct
interred in Norway I$ay cemetery the the Young Hou.eat8.ncl Point n future.</p>

#Observations
The OCR has mashed different advertisements and ads together, most likely as a result of too many small letters being placed together and getting mistaken as one column of text. 
Sometimes it is hard to figure out if the abbreviations are a result of the OCR, or if the original newspaper used them to save space.
Most of the entries are very small - have not yet come across the "large" feature articles we are currently used to
-there are some, but they are written as town meetings/ elections
*perhaps this was before "feature articles as we know them*

This is painstaking work - increases admiration for librarians - so many decisions about where to put breaks - same if I tried to fix up the texts later - each decision has potentially huge consequences
**NO way that just this text can be relied on for any major data mining - it doesn't make much sense**

**It all seems rather subjective actually, and I don't see how I could properly explain every paragraph break I make**

Example: I thought this looked like a list of items on sale, so I separated it from what appears to be prizes for a contest, even though both sections contained indistinguishable characters.

<p>A large, fln«dy-enulpp*d. old established I lut Ion- NON! BETTER IN CANADA.
Bae-lnf##* Kdueatioo at LowofI PoudMe Graduates always eur#p -ful. Write Š oatnlo.ua W. J. Hl.LlOTT, Prlnolphi
Wrappers
Soap
Samuel Rogers, Pres
ïxjîTxwiwrï-six ilau».
DUNNS
BAKING
POWDER</p>
<p>me
as follows:
10 First Prizes, $100 Stsarns' Bicycle,! 1,000 26 Sececd " $25 Odd Watch Bleyolee and Watches given each month 1,625
Total given dur'gyear '97, $19,500
HOW TO For rules and full particulars, iiv tt Š v eee |hfl Toronto Ôlobb
ŠŠŠŠŠ</p>

Also base decisions on careful skimming - sometimes have to take context into consideration.

*A lot of in-depth and long medical potion adverts seem to be mixed up - perhaps they were side-by-side with condensed text
-they are different styles of writing (list, testimony, hyperbole, etc) and refer to different names (Kootenay, KarC Clover Root Tea, Dr. Williams' Medicine, etc)

#August 8, 2017
#Continued TEI work
I'm not sure the ethical restrictions on using this data yet because all the stories about land valuation, medical breakthroughs, huge store sales are all intertwingined - does this impact topic modeling?
This is why people have to be critical and include close reading of their texts - can't just begin with topic modeling

Beginning and end of sentences are good estimates of where page breaks should go, although not everytime. Some lines are started mid-sentence. **Perhaps I can use OpenRefine or something to sort through**

The end section 500 or so lines is written as a fantastical story (I think), so even when it doesn't make much sense, I skim a few lines and if it seems to be the same tone/story, I left it - so it's a big chunk of text

<p>TeacherWhat is that letter? PupilI don't know.
TeacherWhat is it that maker bon
Small boy )son of a manufacturer)-G lucoee.</p>
**I think this is a joke**

It's things like this that make it hard to know how to group them. 
I finished what I hope was a good job with the <p> and </p>

#Fail-log
It was suggested that I not worry about trying to ensure everything was indented properly when adding the <p> and </p> features. There is a plugin that will do that automatically. I installed and tried to run the plugin when I was done the initial markup.
The error indicated that the script couldn't be run because of the extra "<"'s spaced throughout the text because of poor OCR.
I realized I could use regex to find and replace these extra, useless aspects. With my brother's help, I was able to contruct a regex that found the "less than" symbol and whenever it was connected to a character that wasn't "p" or "/", replace it with the html version. This would allow it to be easily read and solve the problem.
*the regex looked like this: <([^p\/]), and was replaced by &gt;\1*

To ensure that it didn't accidently replace the wrong thing, I did it one at a time.

Ran the plugin again, but this time the error identified the unicode and other "irregular" markings, such as "&" as problematic. Should have corrected for the "&" sign before changing the "<". Oh well. 

The plan is to create regex expressions tomorrow to fix these new problems so that it can all be indented and move onto the next step.

#August 9, 2017
Using [RegExr](http://regexr.com/) to try out different regex expressions in order to take out "*" and other similar expressions that seem to cause errors when intenting correctly.

=======
>>>>>>> 7e2636109e7b0e3da0d34b1d6df9c20975be04c3

I first tried "*" to locate the *. It didn't pick it up, so I put in brackets ("*"), but it still didn't work. I recalled the [], so I changed the regex to (["*"]). This worked, although it also picked up the " (quotes) line for some reason.
I created an expression to ignore the " ["*"])(\"). It didn't work and then I realized I could remove the markings in the gedit program using the find and replace function. 
Using the find and replace function I looked up the unicode meaning of the following symbols and replaced "*", "&" with nothing.  With "'",  with """,  with """,  with "'" ,  with """,  with "-",  with nothing because the suggested symbol was a TM and made no sense based on the context.  with nothing because the suggested symbol was a picture of a sword and made absolutely no sense. I made sure to examine each case before replacing it and I found that there was no real option. 

I re-ran the command to automatically align the tags, but it presented me with a lot of errors. I will copy the errors into a separate document and clean them up manually. There are over 300 lines of errors similar to the ones below:

/dev/stdin:97: parser error : StartTag: invalid element name
established at Haley a Station.</p> <p>1 hey are <>lltif remarks that the contin
                                                  ^
/dev/stdin:137: parser error : StartTag: invalid element name
.	<4l,e, wjH, at request,attend Bllcouri.	lent work. “The best	annual	meeting	
 	 ^
/dev/stdin:147: parser error : Specification mandate value for attribute Physician
Physician, Surgeon and
         ^
/dev/stdin:147: parser error : attributes construct error
Physician, Surgeon and
         ^
/dev/stdin:147: parser error : Couldn't find end of Start Tag J line 146
Physician, Surgeon and

*The majority of the errors are caused by stray "<" inputted through the OCR process, so I remove those becuase they are not symbols anyway.In the few instances where the word is clearly visible, I replaced the letter*

Happily, it now comes back with far fewer errors than previously. Now, most of the errors read: 
/dev/stdin:1867: parser error : Premature end of data in tag p line 1541
/dev/stdin:1867: parser error : Premature end of data in tag p line 1171
/dev/stdin:1867: parser error : Premature end of data in tag p line 1171

I forgot to type the "/" in the end of the tag </p>, which makes sense because I copied the opening tag and then just manually added the closing tags.
When I ran it again, it attached them all together. <p></p>. 
My brother thought it was probably an error with the text editor (gedit) I was using, so he sent it through his own more advanced text editor (Atom) and it intented properly. 

**-ethical ramifications**
As discussed in class, my decisions to change the original data in any way is problematic because it alters the original text and may have a strong impact on the digital analysis I run on it. For example, when I decided to remove some incorrectly formatted unicode characters that were disrupting the TEI coding, I was doing something fairly straight-forward on the surface. However, those unicode symbols stood for something and I was making a conscious decision to remove some information from the text that I will theoretically be analysing later on. Since it was in unicode, I have no idea what it actually said, and it could have been important or not. I tried to mitigate the damage I did my making clear notes on what I removed/replaced and why. The hope is that this paradata can be used by future students and historians not only to help them understand the steps I took and my thought-process, but to also gain an understanding of the manipulatings and analysis I ran, so that they can decide how it impacts their planned work and adjust accordingly. When it was obvious (eg. i& was a nice day), I actually replaced the character to read correctly (it was a nice day). However, I did this only when I noticed it and only when it made sense. I have not gone through the whole text as this would have added a new layer of researcher subjectivity and bias to the process. In fact, I wonder if I haven't already done this. I hope whatever changes I have made, when taken into consideration the purpose of this project (to clean and make usable data) is a reasonable level of "risk". Ultimately, the goal would be to run this through OpenRefine too and work through it to clean up some of the data. 

#Open Refine
#Fail-log
I am going to try to use Open Refine to fix some of the spelling and easy to spot errors before I continue with TEI markup.
Not sure if it will work, but my thought is that I can improve the quality of the OCR somewhat and then it will be easier to search or to add to a collection of papers for visualisation later.

I converted the text file into a csv and put it into OpenRefine. It did not make any sense what the results were and looking at it, I realized that there were no headings or anything in the text, so it was using the initial <></> as columns. This is not what I wanted and it was not useful information. Now I remember why I didn't use it in the first place (I had this discussion above). 
Still want to use OpenRefine at some point if possible. Might have to run it through the rest of the TEI first. 

#August 11, 2017
#TEI continued - Encoding/Markup
Now that the file is indented properly, I will encode it and then people will be able to quickly search it for people's names and such. If combined with many different but similar Equity files that have also been similarly coded, this can be visualized and used for much more useful research.

I used some of the Prof's examples for what to code and I created some that I figured would best suit the text, based on the skimming I did when adding the <p> </p> to separate into paragraphs.

#Fail-log
I used the formula below to encode the first name (Cation Thornloe), which appears to be a poorly rendered Captain Thornloe). 
<p> <CationThornloe <key="Thornloe, Cation" from="?" to="?" role="Bishop" ref="none"> </persName>
I received a parse error saying it was improperly formed.
I examined the format and noted that there was an extra ">" before the end tag </persName>. Even though it appeared to follow the format laid out in the template, I modified it to close out the tag:
<p> <CationThornloe/> <key="Thornloe, Cation" from="?" to="?" role="Bishop" ref="none"> </persName>
It then returned a parse error again, saying it was poorly formed. 
I asked for help on our class' slack channel and Dr. Graham responded with the following:
Markup wraps information around text. So <markup>text</markup>
8:45
You are missing the text bit. It's telling you exactly where.
8:46
Move cation  Thornloe to the space between ><
8:51
<p><persName blah blah>Cation Thornloe</persName> was the bishop
I reformatted the way he suggested and the file opened in Firefox without an error, but it did not include any highlighting or any visible "markup".
Prof. Graham asked if I had a stylesheet for the xml file. I did not, nor did I know what it was. He explained and provided a sample. To me, it seems like a legend that formats the tags we put in the xml file.
I'm still confused because the example doesn't seem to match his instructions, so I ask for clarification
"<?xml version="1.0" encoding="UTF-8"?>

<?xml-stylesheet type="text/xsl" href="000style.xsl"?>

<teiCorpus>

right? line 2, that tells the browser to use the stylesheet 000style.xsl to interpret your markup.
dr.graham
9:21 PM
so just put that stylesheet file in the same folder as your xml file, then try reloading
claremaier
9:22 PM
okay, thanks
9:26
there doesn't appear to be "href="000style.xsl"?>
9:26
<?xml version="1.0" encoding="UTF-8"?>
<xsl:stylesheet version="2.0" xmlns:xsl="http://www.w3.org/1999/XSL/Transform">
   <xsl:strip-space elements="*"/>
   <xsl:output method="html" version="4.0" encoding="UTF-8" indent="yes"/>
<xsl:template match="/">"

The professor used my file in his browser and it worked, so I was super confused. Then my brother pointed out that the lines should be in the xml file and I mistakenly thought the professor meant the xsl file. Sometimes it is really annoying to get stuck on such a simple mistake for hours. Tomorrow I will try to encode everything I need, or at least get a significant start on it. I'm going to go through each type of code one at a time, because otherwise I'm quite sure I will get confused between what needs to be coded.
I see this section also includes some research, so that will take up time. Not sure some of the information, such as Grocery sales or specific medicines will be available online, but we'll see. 

I used this style-package
https://github.com/craftingdigitalhistory/module3-wranglingdata/blob/master/tei-hist3907/000style.xsl

#Encoding Legend
Persons <persName key="Last, First" from="YYYY" to="YYYY" role="Occupation" ref="http://www.website.com/webpage.html"> </persName>

Places <placeName key="Sheffield, United Kingdom" ref="http://tools.wmflabs.org/geohack/geohack.php?pagename=Sheffield&params=53_23_01_N_1_28_01_W_type:city_region:GB""> </placeName>

Medicine <medicineName key="Name" from="Business" claim="medProperties" ref="website"> </medicineName>

Sale <saleType key="type" from="Business" to="amount"> </saleType>

#August 12, 2017
#Continuing Encoding the TEI - Focus on Names in First 201 lines
I started by looking up and encoding for the names of people. This required some research using the internet. Sometimes it was easier than other times, based on me having to find alternative ways to spell people's names and try to find historical documents that contain the information I wanted about them. 

For example: Charles M aukk, Esq. on the Board of Directors
I searched Charles Maukk, didn't find any results that were from the right timeframe and/or referenced him being on the board for anything. I added different search phrases to his name and tried different spellings. I finally found several entries for Charles Magee, President of the Bank of Ottawa during the time the paper was written. I noted the correct spelling and began searching for information on him as a person. I learned that he was a dry goods merchant, rich, president of other prestigious groups. I kept looking for his birthdate and death because as such a prominent figure, I was sure it was out there. *Here is where if I was doing this seriously as part of a larger research project, I would have noted the name for further, primary source research within a library or archives setting*. I did find his information in an online meeting document from the City of Ottawa in 2014 regarding Heritage Buildings. It was about some houses Magee built. *In the process, I learned about the history of the Bank of Ottawa and its merger with the Bank of Nova Scotia, as well as other interesting historical facts. I also can see just how time-consuming this project could be if it was undertaken most professionally*

The information I include in the brief description of the person also needs to be problematized because each person could have at least a full essay written about them, their politics, significance to early Ottawa history, influence at the Bank, trade influence, religious and social views, etc. 
Each decision about what to include should be explicitly given, although I am not sure where it would be most appropriate to do so or how that type of paradata should be shared. For example, in looking up George Hay, I found one biography for him that seemed to give a rather complete history of his interactions with several different aspects of life. I chose not to look at any addition resources after briefly searching to make sure George Hay was the person I was looking for (context included connection to the Bank of Ottawa, that he was wealthy and influential). I also chose to exclude some of the information about his complex political ties more detailed religious leanings. I did include that he was the leader of the Ottawa Bible Society because it seemed to include his broader position within the religious circles about Ottawa. I also included it because as a person of faith myself, I found myself identifying with him on this level. *While this admission is important for transparency, I do see how my bias could be problematic because it influences how those who use my source learn, interact, further research, and interact with George Hay. However, I do not see a solution, other than to be honest about the processes I took and the decisions I made, because to not do so would be problematic, and as a subjective human, there is no way that the information could be presented in a non-subjective manner. Perhaps this is the danger of digital sources and the type of search function I am writing: people are so used to searching things on Google that they incorrectly assume Google (and every other type of search) is objective. It is not, as even Google recently got fined for preferencing its own products in its search function. Just as we are critical about the value of primary sources, we need to be critical about what we find and how we find online sources. I just don't know the exact most appropriate format or method, but I will try to highlight within this document any decisions I made that seem most significant in creating a bias within the text.*

When I could not be sure that the person whose information I found matched the person in the Equity article, I chose to not include the information. I did this to maintain a level of good scholarship and not present potentially misleading information that future researchers might take at face value. This decision applied to people who were in the paper, but were not necessarily "big players", such as local shopkeepers. If I were to focus on this project as a significant research project, I would have taken the time to go to local registry offices and done cross-referencing between numerous newspaper articles, school registries, church registries, property titles, etc, in order to determine who people are. This would be the next step if someone were going to focus on this newspaper entry as part of a larger effort to create an Equity-wide searchable database. 

**I have spent most of the day looking up information on the people mentioned in the first 200 lines of the Equity article. At this point, it may be a better idea to only look at the first 400 lines and fully encode that, using it as an example of what the completed text would look like.**

*Note: While I did my best to highlight all the names that appeared in the first 201 lines, I may have missed a few - this oversight is entirely mine. I also did my best (by copying) the same tags for the same people, but again, some oversight may have slipped through. This is something people should be aware of when using these sources - it does not absolve people from doing their own critical thinking and fact checking*

#Fail-log
I just double-checked to make sure my file worked and I received the parsing error again. Turns out the "&" symbol in the URL was throwing it off. Couldn't figure out how to fix it, so I removed it and [replaced](http://www.res.parl.gc.ca/parlinfo/Files/Parliamentarian.aspx?Item=e04a4ab4-fa00-4451-904e-6c261abd68c0&Language=E&Section=ALL) with ParliamentofCanadawebsite, Murray, Thomas. I'm happy to fix it once I figure out how.
I had the same problem with a GoogleBooks [search](https://books.google.ca/books?id=IZFXAAAAMAAJ&pg=PA248&lpg=PA248&dq=sj+mcnally+ottawa&source=bl&ots=jUoOqXoBYi&sig=55PDw9IKjON8xpqFN8qJmbzDljA&hl=en&sa=X&ved=0ahUKEwi0tMWovdLVAhUL7YMKHbYgBWwQ6AEIPzAF#v=onepage&q=sj%20mcnally%20ottawa&f=false) 
Canada Medical Record, Volume XXIV, Oct., 1895, to Sept., 1896

When I copied one of the tags near the beginning (to use by quickly modifying), I also grabbed the sentence several times: before it and this was detected as incorrect parsing by my browser. I had to go through and remove this sentence </persName> ex M. P., will grace the Mayor's chair, in the town of Pembroke for the current year. 

**XML Parsing Error:** mismatched tag. Expected: </p>.
Location: file:///home/clare/School/Final_EquityProject/TEI_OCR_Tagging.xml
Line Number 829, Column 15:            </body>
--------------^
**Not sure why this parsing error. Could be that it doesn't like the new alignment with all the added tags**

#August 13, 2017
#Continuing Encoding the TEI

#Fail-log
I was able to fix the two URLs listed above. The xml wouldn't read the "&" symbol, so my brother suggested I replace it with "&amp;" which is essentially the same thing, but is recognized by xml. 
Similarly, the last parsing error with the mismatched tag was fixed by looking through the file and carefully fixing the few tags that I hadn't closed properly. 

#August 14, 2017
#Places
<placeName key="Sheffield, United Kingdom" ref="http://tools.wmflabs.org/geohack/geohack.php?pagename=Sheffield&params=53_23_01_N_1_28_01_W_type:city_region:GB""> </placeName>

This went well. I decided that by places I meant geographical cities and regions and not "Shawville Skating Rink" or businesses. I did this because I did not see the value in encoding such obvious places and others, such as McGuire's Grocery, were not searchable using Google. I may revisit this decision later. 

#Medicine
Medicine <medicineName key="Name" from="Business" claim="medProperties" ref="website"> </medicineName>

*I have decided to highlight medicines within the text because it seemed to me that I was seeing several different medicine advertisements throughout the paper and I wanted to know how many, and exactly what they promised. This is a personal interest because as a person who watched Little House on the Prairie, I often learned about "phony" medicines that were mostly alcohol. I wanted to explore this further and see if it was widespread/common practice.*

I used the pdf copy of the paper to find all the mentions of newspapers

*I will do the actual research tomorrow at work*

#Sales/Items
Sale <saleType key="type" from="Business" to="amount"> </saleType>

I am not sure exactly what/how I want to approach this category - I can either do it by store/owner or by type of item. Each has advantages and disadvantages. The advantage of itemized list lets the researcher notice what items are commonly for sale and if crossed with data from several months/years/seasons - did it change? Learn about seasonal fruits and vegetables and if it changed with the advent of further distance transportation or new imports. Is there a difference in the clothes or medicines being sold? I can't do that with this project because I'm focused on one newspaper, but if would be a good future use of this material.

#By Salesman

#Item
**DO I include the list of medicines here** Discuss


#Research Questions
**How often are well-known people vs. regular people mentioned?**
**How might this speak to the function or readership of the paper?**

**How frequently are locations mentioned? Does this speak to the relative "world" they lived in? E.g. closer contact with locals - what they were interested in?**

**Are some items sold more than others? What and why?**



